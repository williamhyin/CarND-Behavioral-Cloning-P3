<!DOCTYPE html>
<html>
<head>
<title>my_writeup</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<h1><strong>Behavioral Cloning</strong></h1>
<h2>Writeup Template</h2>
<h3>You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.</h3>
<hr />
<p><strong>Behavioral Cloning Project</strong></p>
<p>The goals / steps of this project are the following:</p>
<ul>
<li>Use the simulator to collect data of good driving behavior</li>
<li>Build, a convolution neural network in Keras that predicts steering angles from images</li>
<li>Train and validate the model with a training and validation set</li>
<li>Test that the model successfully drives around track one without leaving the road</li>
<li>Summarize the results with a written report</li>
</ul>
<h2>Rubric Points</h2>
<h3>Here I will consider the <a href="https://review.udacity.com/#!/rubrics/432/view">rubric points</a> individually and describe how I addressed each point in my implementation.</h3>
<hr />
<h3>Files Submitted &amp; Code Quality</h3>
<h4>1. Submission includes all required files and can be used to run the simulator in autonomous mode</h4>
<p>My project includes the following files:</p>
<ul>
<li>final_ model.py, final_ model.html and final_ model.ipynb containing the script to create and train the model</li>
<li>drive.py for driving the car in autonomous mode</li>
<li>model_cnn1.h5 containing a trained convolution neural network </li>
<li>my_writeup.md/html summarizing the results</li>
</ul>
<h4>2. Submission includes functional code</h4>
<p>Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
<code>sh
python drive.py model.h5</code></p>
<h4>3. Submission code is usable and readable</h4>
<p>The final_ model.ipynb/py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.</p>
<h3>Model Architecture and Training Strategy</h3>
<h4>1. Model Overview</h4>
<p>I decided to use NVIDA model, which is proved to be more powerful. The input of my model is (160x30x3). After that the data is normalized in the model using a Keras lambda layer.</p>
<pre><code>model.add(Lambda(lambda x: x / 255 - 0.5, input_shape=input_shape))
</code></pre>

<p>But there are many unnecessary information in the images. So we need to crop and resize the image to augmente data.</p>
<pre><code>model.add(Cropping2D(cropping=((70, 25), (0, 0))))
</code></pre>

<p><img src="https://i.imgur.com/YhKz7yG.png" /></p>
<h4>2. Loading and spliting Data</h4>
<ul>
<li>
<p>I uesd Udacity simulator to record data. I use the mouse to control the movement direction of the vehicle, so that the value of the steering angle changes smoothly. There are 4 anticlockwise loop, 3 clockwise loop.</p>
</li>
<li>
<p>Because CV2 reads an image in BGR, so we  need to be converted to RGB, which can be processed in drive.py. </p>
</li>
</ul>
<p>codeï¼š</p>
<pre><code>def load_image(path):
    # CV2 reads an image in BGR, which need to be converted to RGB 
    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)
    return img
</code></pre>

<ul>
<li>Since we have 3 camera image, but the steering angle is associated with center camera, so I need to introduce a correction factor for left(0.25) and right(-0.25) images. Then I combined images and steering angle from 3 camera to augment my dataset. Before that we need split the original center dataset to train-samples and validation samples. I decided to keep 15% of the data in Validation Set and remaining in Training Set.</li>
</ul>
<p>code:</p>
<pre><code>def split_data(samples, test_size):
    # split data to two parts: train-sample and validation-sample
    train_samples, validation_samples = train_test_split(samples, test_size=test_size)
    return train_samples, validation_samples
</code></pre>

<h4>3. Preprocessing</h4>
<ul>
<li>filp: I decided to flip the image horizontally and adjust steering angle accordingly to balance the distribution of left steering and right steering, and increse the numbers of my dataset samples.</li>
</ul>
<p>code: </p>
<pre><code>def flip(image, angle):
    # flip the image vertical 
    new_image = cv2.flip(image, 1)
    # negative angle
    new_angle = angle * (-1)
    return new_image, new_angle
</code></pre>

<p>Distribution of raw center steering is shown below:</p>
<p><img src="https://i.imgur.com/Impj9de.png" /></p>
<ul>
<li>brightness: I decided to use random brightness changes to make my model can adapt different bright condition.</li>
</ul>
<p>code:</p>
<pre><code>def random_brightness(image):
    # Convert 2 HSV colorspace from RGB colorspace
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
    # Generate new random brightness
    random_bright = random.uniform(0.3, 1.0) 
    hsv[:, :, 2] = random_bright * hsv[:, :, 2]
    # Convert back to RGB colorspace
    new_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return new_img
</code></pre>

<ul>
<li>crop and resize images: There are many unnecessary information in the images. So we need to crop and resize the image to augmente data. (but I use keras.layers.Cropping2D to crop and resize images instead of cv.resize function.</li>
</ul>
<p>code:</p>
<pre><code>def crop_resize(image):
    # resize the image to throw unnecessary information
    # original 320x160x3
    cropped = cv2.resize(image[60:140, :], (64, 64))
    return cropped
</code></pre>

<p>The example-results of  data loading and preprocessing are shown below:</p>
<p>The 1st, 3rd and 5th columns are raw center, left and right images.
The 2nd, 4th and 6th columns are augmented (flip, crop-resize, random brightness) images.</p>
<p><img src="https://i.imgur.com/lpmTK6L.png" />
<img src="https://i.imgur.com/ML6hsef.png" />
<img src="https://i.imgur.com/gwuasG4.png" />
<img src="https://i.imgur.com/Z6c7NVR.png" />
<img src="https://i.imgur.com/u1apHeM.png" />
<img src="https://i.imgur.com/aqWnvoE.png" />
<img src="https://i.imgur.com/MrgKznQ.png" />
<img src="https://i.imgur.com/pEbMoVw.png" /></p>
<h4>4. Final model</h4>
<p>The model includes 5 convolution layers and 5 dense Layers. RELU layers to introduce nonlinearity, and the data is normalized in the model using a Keras lambda layer. </p>
<p>The model contains dropout layers (0.5)in order to reduce overfitting . </p>
<p>The model was trained and validated on different data sets to ensure that the model was not overfitting . The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.</p>
<p><img src="https://i.imgur.com/Gaf0MAo.png" /></p>
<p>Code:</p>
<pre><code>def cnn_1(_):

    samples = reader_csv(PATH)
    train_samples, validation_samples = split_data(samples, test_size=SPLIT_SIZE)
    # get train-sample batch and validation sample batch
    train_generator = generator_data(train_samples, batch_size=BTACH_SIZE, aug=True)
    validation_generator = generator_data(validation_samples, batch_size=BTACH_SIZE, aug=False)

    input_shape = (160, 320, 3) # original image size 
    model = Sequential()
    model.add(Lambda(lambda x: x / 255 - 0.5, input_shape=input_shape))
    model.add(Cropping2D(cropping=((70, 25), (0, 0)))) # resize image with Cropping2D fuction, which is more faster
    model.add(Conv2D(24,kernel_size= (5, 5), padding='valid', strides=(2, 2), kernel_regularizer=l2(0.001)))
    model.add(Activation('relu'))

    model.add(Conv2D(36, kernel_size=(5, 5), padding='valid', strides=(2, 2), kernel_regularizer=l2(0.001)))
    model.add(Activation('relu'))

    model.add(Conv2D(48, kernel_size=(5, 5), padding='valid', strides=(2, 2), kernel_regularizer=l2(0.001)))

    model.add(Activation('relu'))

    model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2, 2), kernel_regularizer=l2(0.001)))
    model.add(Activation('relu'))

    model.add(Conv2D(64,kernel_size= (3, 3), padding='valid', strides=(2, 2), kernel_regularizer=l2(0.001)))
    model.add(Activation('relu'))

    model.add(Flatten())
    model.add(Dense(80, kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))
    model.add(Dense(40, kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))
    model.add(Dense(16, kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))
    model.add(Dense(10, kernel_regularizer=l2(0.001)))
    model.add(Dense(1, kernel_regularizer=l2(0.001)))

    adam = Adam(lr=0.0001)
    model.compile(optimizer=adam, loss='mse',metrics=['accuracy']) 
    model.summary()
    history_object =model.fit_generator(train_generator, validation_data=validation_generator,
                        steps_per_epoch=len(train_samples), epochs=EPOCHS,
                        validation_steps=len(validation_samples), verbose=1)

    print('Done Training')    
    # Save model
    model.save(&quot;model_cnn1.h5&quot;)
    print(&quot;Saved model to disk&quot;)
    print(history_object.history.keys())

    ## plot the training and validation loss for each epoch
    plt.plot(history_object.history['loss'])
    plt.plot(history_object.history['val_loss'])
    plt.title('model mean squared error loss')
    plt.ylabel('mean squared error loss')
    plt.xlabel('epoch')
    plt.legend(['training set', 'validation set'], loc='upper right')
    plt.show()
</code></pre>

<p>The training results are shown below:</p>
<p><img src="https://i.imgur.com/7LlvM5N.png" /></p>
<p>AS you see, the accuracy didnot decrease and the loss is small . I think I can't get better results unless I find more better paramaters or add more data samples.</p>
<h4>5. Model parameter tuning</h4>
<p>The model used an adam optimizer, so the learning rate was not tuned manually. And I use l2-regulation.</p>
<p>CORRECTION_FACTOR = 0.25</p>
<p>EPOCHS = 5</p>
<p>SPLIT_SIZE = 0.15</p>
<p>BTACH_SIZE = 32</p>
<p>PATH = &quot;./all_data/&quot;</p>
<p>l2-kernel_regularizer=0.001</p>
<p>Adam-learning rate=0.0001</p>
<h4>5. Output Video</h4>
<iframe width="560" height="315" src="https://www.youtube.com/embed/iE2Q_QQVEr0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
